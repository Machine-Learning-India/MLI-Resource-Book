# Natural Language Processing



* [A curated list of speech and natural language processing resources](https://github.com/edobashira/speech-language-processing) A curated list of speech and natural language processing resources that can be useful for beginners.
* [Understanding Natural Language with Deep Neural Networks Using Torch](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/) Language is the medium of human communication. Giving machines the ability to learn and understand language enables products and possibilities that are not imaginable today.
* [tf-idf explained](http://michaelerasm.us/post/tf-idf-in-10-minutes/) What is TF-IDF? The 10 minute guide
* [Interesting Deep Learning NLP Projects Stanford](http://cs224d.stanford.edu/reports.html), [Website](http://cs224d.stanford.edu/) Deep Learning for Natural Language Processing
* [The Stanford NLP Group](https://nlp.stanford.edu/) The Natural Language Processing Group at Stanford University is a team of faculty, postdocs, programmers and students who work together on algorithms that allow computers to process and understand human languages.
* [NLP from Scratch \| Google Paper](https://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/35671.pdf) Natural Language Processing \(almost\) from Scratch
* [Graph Based Semi Supervised Learning for NLP](http://graph-ssl.wdfiles.com/local--files/blog%3A_start/graph_ssl_acl12_tutorial_slides_final.pdf) Graph-based Semi-Supervised Learning Algorithms for NLP
* [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model)
  * [Classification text with Bag of Words](http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/) 
* Topic Modeling
  * [Topic Modeling Wikipedia](https://en.wikipedia.org/wiki/Topic_model)
  * [**Probabilistic Topic Models Princeton PDF**](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf)
  * [LDA Wikipedia](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation), [LSA Wikipedia](https://en.wikipedia.org/wiki/Latent_semantic_analysis), [Probabilistic LSA Wikipedia](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis)
  * [What is a good explanation of Latent Dirichlet Allocation \(LDA\)?](https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation)
  * [**Introduction to LDA**](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/), [Another good explanation](http://confusedlanguagetech.blogspot.in/2012/07/jordan-boyd-graber-and-philip-resnik.html)
  * [The LDA Buffet - Intuitive Explanation](http://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/)
  * [Your Guide to Latent Dirichlet Allocation \(LDA\)](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d)
  * [Difference between LSI and LDA](https://www.quora.com/Whats-the-difference-between-Latent-Semantic-Indexing-LSI-and-Latent-Dirichlet-Allocation-LDA)
  * [Original LDA Paper](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf)
  * [alpha and beta in LDA](http://datascience.stackexchange.com/questions/199/what-does-the-alpha-and-beta-hyperparameters-contribute-to-in-latent-dirichlet-a)
  * [Intuitive explanation of the Dirichlet distribution](https://www.quora.com/What-is-an-intuitive-explanation-of-the-Dirichlet-distribution)
  * [topicmodels: An R Package for Fitting Topic Models](https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf)
  * [Topic modeling made just simple enough](https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/)
  * [Online LDA](http://alexminnaar.com/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html), [Online LDA with Spark](http://alexminnaar.com/distributed-online-latent-dirichlet-allocation-with-apache-spark.html)
  * [LDA in Scala](http://alexminnaar.com/latent-dirichlet-allocation-in-scala-part-i-the-theory.html), [Part 2](http://alexminnaar.com/latent-dirichlet-allocation-in-scala-part-ii-the-code.html)
  * [Segmentation of Twitter Timelines via Topic Modeling](https://alexisperrier.com/nlp/2015/09/16/segmentation_twitter_timelines_lda_vs_lsa.html)
  * [Topic Modeling of Twitter Followers](http://alexperrier.github.io/jekyll/update/2015/09/04/topic-modeling-of-twitter-followers.html)
  * [Multilingual Latent Dirichlet Allocation \(LDA\)](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA). \([Tutorial here](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA/blob/master/Multilingual-LDA-Pipeline-Tutorial.ipynb)\)
  * [Deep Belief Nets for Topic Modeling](https://github.com/larsmaaloee/deep-belief-nets-for-topic-modeling)
  * [Gaussian LDA for Topic Models with Word Embeddings](http://www.cs.cmu.edu/~rajarshd/papers/acl2015.pdf)
  * Python
    * [Series of lecture notes for probabilistic topic models written in ipython notebook](https://github.com/arongdari/topic-model-lecture-note)
    * [Implementation of various topic models in Python](https://github.com/arongdari/python-topic-model) 
* word2vec
  * [Google word2vec](https://code.google.com/archive/p/word2vec)
  * [Bag of Words Model Wiki](https://en.wikipedia.org/wiki/Bag-of-words_model)
  * [word2vec Tutorial](https://rare-technologies.com/word2vec-tutorial/)
  * [A closer look at Skip Gram Modeling](http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf)
  * [Skip Gram Model Tutorial](http://alexminnaar.com/word2vec-tutorial-part-i-the-skip-gram-model.html), [CBoW Model](http://alexminnaar.com/word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html)
  * [Word Vectors Kaggle Tutorial Python](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors), [Part 2](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors)
  * [Making sense of word2vec](http://rare-technologies.com/making-sense-of-word2vec/)
  * [word2vec explained on deeplearning4j](http://deeplearning4j.org/word2vec.html)
  * [Quora word2vec](https://www.quora.com/How-does-word2vec-work)
  * [Other Quora Resources](https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures-in-laymans-terms), [2](https://www.quora.com/What-is-the-difference-between-the-Bag-of-Words-model-and-the-Continuous-Bag-of-Words-model), [3](https://www.quora.com/Is-skip-gram-negative-sampling-better-than-CBOW-NS-for-word2vec-If-so-why)
  * [word2vec, DBN, RNTN for Sentiment Analysis](http://deeplearning4j.org/zh-sentiment_analysis_word2vec.html)
* Text Clustering
  * [How string clustering works](http://stackoverflow.com/questions/8196371/how-clustering-works-especially-string-clustering)
  * [Levenshtein distance for measuring the difference between two sequences](https://en.wikipedia.org/wiki/Levenshtein_distance)
  * [Text clustering with Levenshtein distances](http://stackoverflow.com/questions/21511801/text-clustering-with-levenshtein-distances)
* Text Classification
  * [Classification Text with Bag of Words](http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/)
* Named Entity Recognitation
  * [Stanford Named Entity Recognizer \(NER\)](https://nlp.stanford.edu/software/CRF-NER.shtml)
  * [Named Entity Recognition: Applications and Use Cases- Towards Data Science](https://towardsdatascience.com/named-entity-recognition-applications-and-use-cases-acdbf57d595e)
* [Language learning with NLP and reinforcement learning](http://blog.dennybritz.com/2015/09/11/reimagining-language-learning-with-nlp-and-reinforcement-learning/)
* [Kaggle Tutorial Bag of Words and Word vectors](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words), [Part 2](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors), [Part 3](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors)
* [What would Shakespeare say \(NLP Tutorial\)](https://gigadom.wordpress.com/2015/10/02/natural-language-processing-what-would-shakespeare-say/)
* [A closer look at Skip Gram Modeling](http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf)

